#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‡ªå‹•æŠ“å– HKJC æœ€è¿‘ä¸€å ´è³½äº‹ï¼ˆæ”¯æ´éè³½é¦¬æ—¥ï¼‰
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import argparse
import logging
import os
from datetime import datetime, timedelta

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# æ”¹ç”¨ã€Œæœ¬é€±è³½ç¨‹ã€ä½œç‚ºå…¥å£ï¼ˆæ›´ç©©å®šï¼‰
WEEKLY_RACE_URL = "https://racing.hkjc.com/racing/information/Chinese/Racing/LocalResultsAll.aspx"

def get_next_race_info():
    """å¾æœ¬é€±è³½ç¨‹ä¸­æ‰¾å‡ºæœ€è¿‘ä¸€å ´æœªé–‹è·‘çš„è³½äº‹"""
    logger.info("æ­£åœ¨æŸ¥è©¢ HKJC æœ€è¿‘ä¸€å ´è³½äº‹...")
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }
    
    try:
        response = requests.get(WEEKLY_RACE_URL, headers=headers, timeout=10)
        response.raise_for_status()
        response.encoding = 'utf-8'
    except Exception as e:
        logger.error(f"âŒ ç„¡æ³•é€£æ¥ HKJC è³½ç¨‹é é¢: {e}")
        return None
    
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # æ‰¾æ‰€æœ‰è³½äº‹é€£çµï¼ˆåŒ…å« RaceDate, Venue, RaceNoï¼‰
    race_links = soup.find_all('a', href=True)
    future_races = []
    
    for link in race_links:
        href = link['href']
        if 'DisplayRaceCard.aspx' in href and 'RaceDate=' in href:
            # è§£æåƒæ•¸
            params = {}
            for part in href.split('&'):
                if '=' in part:
                    key, val = part.split('=', 1)
                    params[key] = val
            
            if 'RaceDate' in params and 'Venue' in params:
                # æª¢æŸ¥æ˜¯å¦ç‚ºæœªä¾†æˆ–ä»Šå¤©çš„è³½äº‹
                race_date = params['RaceDate']
                try:
                    race_dt = datetime.strptime(race_date, "%Y/%m/%d")
                    today = datetime.now()
                    if race_dt.date() >= today.date():
                        # æ‰¾åˆ°ç¬¬ä¸€å ´å°±è¿”å›ï¼ˆæœ€æ–°ä¸€å ´ï¼‰
                        return {
                            'date': race_date.replace('/', ''),
                            'venue': params['Venue'],
                            'race_no': '1'  # å¾ç¬¬ 1 å ´é–‹å§‹
                        }
                except:
                    continue
    
    logger.warning("âš ï¸ æœªæ‰¾åˆ°è¿‘æœŸè³½äº‹ï¼ˆå¯èƒ½æœ¬é€±ç„¡è³½é¦¬ï¼‰")
    return None

def fetch_race_card(date, venue, race_no):
    """æŠ“å–å–®å ´è³½äº‹é¦¬åŒ¹è³‡æ–™ï¼ˆå…¼å®¹æ–°èˆŠ HTMLï¼‰"""
    url = f"https://racing.hkjc.com/racing/information/Chinese/Racing/DisplayRaceCard.aspx?RaceDate={date[:4]}/{date[4:6]}/{date[6:]}&Venue={venue}&RaceNo={race_no}"
    headers = {"User-Agent": "Mozilla/5.0"}
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        response.encoding = 'utf-8'
    except Exception as e:
        logger.error(f"âŒ ç„¡æ³•è¼‰å…¥è³½äº‹é é¢ ({url}): {e}")
        return []
    
    soup = BeautifulSoup(response.text, 'html.parser')
    horses = []
    
    # æ–°ç‰ˆè¡¨æ ¼é¸æ“‡å™¨ï¼ˆ2024â€“2026 å¹´å¸¸ç”¨çµæ§‹ï¼‰
    tables = soup.find_all('table', class_='table_bd_dr')
    if not tables:
        logger.warning("âš ï¸ æœªæ‰¾åˆ°é¦¬åŒ¹è¡¨æ ¼ï¼ˆå¯èƒ½é é¢çµæ§‹è®Šæ›´ï¼‰")
        return []
    
    # å–ç¬¬ä¸€å€‹è¡¨æ ¼ï¼ˆé€šå¸¸å°±æ˜¯é¦¬åŒ¹åå–®ï¼‰
    table = tables[0]
    rows = table.find_all('tr')[2:]  # è·³éå‰å…©è¡Œï¼ˆæ¨™é¡Œï¼‰
    
    for row in rows:
        cols = row.find_all('td')
        if len(cols) < 8:
            continue
        
        try:
            draw = cols[0].get_text(strip=True)  # æª”ä½
            horse_name = cols[3].find('a').get_text(strip=True) if cols[3].find('a') else cols[3].get_text(strip=True)
            jockey = cols[5].get_text(strip=True)
            trainer = cols[6].get_text(strip=True)
            weight = cols[7].get_text(strip=True)
            
            # æ¸…ç†è² ç£…ï¼ˆç§»é™¤éæ•¸å­—ï¼‰
            weight = ''.join(filter(str.isdigit, weight)) or '120'
            
            horses.append({
                'æª”ä½': draw,
                'é¦¬å': horse_name,
                'é¨å¸«': jockey,
                'ç·´é¦¬å¸«': trainer,
                'å¯¦éš›è² ç£…': int(weight)
            })
        except Exception as e:
            logger.debug(f"è·³éç„¡æ•ˆè¡Œ: {e}")
            continue
    
    return horses

def main(output_dir: str):
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. æ‰¾æœ€è¿‘ä¸€å ´è³½äº‹
    race_info = get_next_race_info()
    if not race_info:
        logger.error("âŒ ç„¡æ³•å–å¾—è¿‘æœŸè³½äº‹è³‡è¨Š")
        return
    
    date, venue = race_info['date'], race_info['venue']
    logger.info(f"âœ… æ‰¾åˆ°è³½äº‹: {date} @ {venue}")
    
    # 2. æŠ“å–æ‰€æœ‰å ´æ¬¡ï¼ˆæœ€å¤š 12 å ´ï¼‰
    all_races = []
    for race_no in range(1, 13):
        logger.info(f"æŠ“å–ç¬¬ {race_no} å ´...")
        horses = fetch_race_card(date, venue, str(race_no))
        if not horses:
            break  # ç„¡æ›´å¤šå ´æ¬¡
        
        # è‡¨æ™‚è³ ç‡ï¼ˆå›  XML API ä¸ç©©å®šï¼Œå…ˆç”¨é è¨­å€¼ï¼‰
        for i, h in enumerate(horses):
            h['ç¨è´è³ ç‡'] = 999.0  # è¡¨ç¤ºæœªæä¾›
        
        df = pd.DataFrame(horses)
        filename = f"{output_dir}/hkjc_{date}_{venue}_race{race_no}.csv"
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        logger.info(f"âœ… å„²å­˜: {filename}")
        all_races.append(filename)
        time.sleep(1)  # é¿å…éå¿«è«‹æ±‚
    
    if all_races:
        logger.info(f"ğŸ‰ å…±æŠ“å– {len(all_races)} å ´è³½äº‹ï¼")
        # åŒæ™‚ç”Ÿæˆä¸€å€‹åˆä½µæ–‡ä»¶æ–¹ä¾¿æ¸¬è©¦
        sample_file = all_races[0]
        os.system(f"cp {sample_file} {output_dir}/next_race.csv")
        logger.info("ğŸ“ å·²è¤‡è£½ç¬¬ä¸€å ´ç‚º next_race.csvï¼ˆä¾› predict.py ä½¿ç”¨ï¼‰")
    else:
        logger.warning("âš ï¸ æœªæŠ“å–åˆ°ä»»ä½•è³½äº‹")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--output", default="data/predictions")
    args = parser.parse_args()
    main(args.output)