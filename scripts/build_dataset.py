#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Build a clean, structured dataset from HKJC racing result Excel files.
Handles column mapping and mixed-type data safely.
"""

import pandas as pd
import numpy as np
import os
import argparse
from pathlib import Path
import logging

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# ===== æ¬„ä½æ˜ å°„è¡¨ =====
COLUMN_MAPPING = {
    'åæ¬¡': 'finish_position',
    'é¦¬è™Ÿ': 'horse_number',
    'é¦¬å': 'horse_name',
    'é¨å¸«': 'jockey',
    'ç·´é¦¬å¸«': 'trainer',
    'å¯¦éš› è² ç£…': 'actual_weight',
    'æ’ä½ é«”é‡': 'declared_weight',
    'æª”ä½': 'draw',
    'ç¨è´ è³ ç‡': 'win_odds',
    # å…¶ä»–å¯èƒ½å¯«æ³•
    'å¯¦éš›è² ç£…': 'actual_weight',
    'æ’ä½é«”é‡': 'declared_weight',
    'ç¨è´è³ ç‡': 'win_odds',
}

def flatten_columns(columns):
    """è™•ç† MultiIndex æ¬„ä½"""
    flattened = []
    for i, col in enumerate(columns):
        if isinstance(col, tuple):
            clean_parts = []
            for c in col:
                if pd.notna(c) and str(c).strip() and 'Unnamed' not in str(c):
                    clean_parts.append(str(c).strip())
            name = "_".join(clean_parts) if clean_parts else f"col_{i}"
        else:
            name = str(col).strip() if pd.notna(col) else f"col_{i}"
        flattened.append(name)
    return flattened

def load_all_races(raw_dir: str):
    all_races = []
    xlsx_files = list(Path(raw_dir).rglob("*.xlsx"))
    logger.info(f"ğŸ” æ‰¾åˆ° {len(xlsx_files)} å€‹ Excel æª”æ¡ˆ")
    
    for file_path in xlsx_files:
        try:
            xls = pd.ExcelFile(file_path)
            for sheet_name in xls.sheet_names:
                if "Race" in str(sheet_name):
                    df = pd.read_excel(xls, sheet_name=sheet_name, header=0)
                    df.columns = flatten_columns(df.columns)
                    df = df.rename(columns=COLUMN_MAPPING)  # æ‡‰ç”¨æ˜ å°„
                    df["source_file"] = file_path.stem
                    df["race_sheet"] = sheet_name
                    all_races.append(df)
        except Exception as e:
            logger.warning(f"âš ï¸ è·³é {file_path.name}: {e}")
            continue
    
    if not all_races:
        raise ValueError("æ²’æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„ Race è¡¨æ ¼ï¼")
    combined = pd.concat(all_races, ignore_index=True)
    logger.info(f"ğŸ“Š åˆä½µå®Œæˆï¼šå…± {len(combined)} ç­†è³½é¦¬è¨˜éŒ„")
    return combined

def clean_data(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    
    REQUIRED_COLUMNS = [
        'finish_position', 'horse_name', 'jockey', 'trainer',
        'actual_weight', 'declared_weight', 'draw', 'win_odds'
    ]
    cols_to_keep = [col for col in REQUIRED_COLUMNS if col in df.columns]
    cols_to_keep += ['source_file', 'race_sheet']
    df = df[cols_to_keep].copy()
    
    # æ•¸å€¼æ¸…æ´—
    for col in ['actual_weight', 'declared_weight', 'draw', 'win_odds']:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
    
    # åæ¬¡è™•ç†
    if 'finish_position' not in df.columns:
        raise KeyError("âŒ æ‰¾ä¸åˆ° 'finish_position' æ¬„ä½ï¼è«‹æª¢æŸ¥ COLUMN_MAPPING")
    
    df['finish_position'] = (
        df['finish_position'].astype(str).str.extract(r'(\d+)')[0].astype(float)
    )
    df = df.dropna(subset=['finish_position'])
    df['is_top3'] = df['finish_position'] <= 3.0
    
    # ç”Ÿæˆ ID
    for col in ['horse_name', 'jockey', 'trainer']:
        if col in df.columns:
            df[f"{col}_id"] = pd.Categorical(df[col]).codes
    
    return df

def main(raw_dir: str, output_dir: str):
    os.makedirs(output_dir, exist_ok=True)
    df_raw = load_all_races(raw_dir)
    df_clean = clean_data(df_raw)
    
    parquet_path = os.path.join(output_dir, "hkjc_racing_data.parquet")
    csv_path = os.path.join(output_dir, "hkjc_racing_data.csv")
    
    df_clean.to_csv(csv_path, index=False, encoding='utf-8-sig')
    df_clean.to_parquet(parquet_path, index=False)
    
    logger.info(f"âœ… CSV å·²å„²å­˜è‡³: {csv_path}")
    logger.info(f"âœ… Parquet å·²å„²å­˜è‡³: {parquet_path}")
    logger.info(f"ğŸ“ˆ æœ€çµ‚è¨˜éŒ„æ•¸ï¼š{len(df_clean)}")
    logger.info(f"ğŸ¯ å…¥ä½æ¯”ä¾‹ï¼š{df_clean['is_top3'].mean():.2%}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--raw", default="../data/raw")
    parser.add_argument("--output", default="../data/processed")
    args = parser.parse_args()
    main(args.raw, args.output)