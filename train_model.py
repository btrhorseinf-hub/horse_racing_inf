# train_xgb_model.py â€”â€” å« Optuna è¶…åƒæ•¸èª¿å„ª + ç‰¹å¾µé‡è¦æ€§åˆ†æ

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import roc_auc_score
from xgboost import XGBClassifier
import joblib
import optuna
import matplotlib.pyplot as plt

# å…¨åŸŸè®Šæ•¸ï¼ˆä¾› objective å‡½æ•¸ä½¿ç”¨ï¼‰
X_global = None
y_global = None

def objective(trial):
    """Optuna æœ€ä½³åŒ–ç›®æ¨™å‡½æ•¸ï¼šæœ€å¤§åŒ– CV AUC"""
    params = {
        "n_estimators": trial.suggest_int("n_estimators", 100, 500),
        "max_depth": trial.suggest_int("max_depth", 3, 8),
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
        "subsample": trial.suggest_float("subsample", 0.6, 1.0),
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
        "reg_alpha": trial.suggest_float("reg_alpha", 0, 10),
        "reg_lambda": trial.suggest_float("reg_lambda", 0, 10),
        "gamma": trial.suggest_float("gamma", 0, 5),
        "min_child_weight": trial.suggest_int("min_child_weight", 1, 10),
        "random_state": 42,
        "eval_metric": "logloss",
        "use_label_encoder": False,
    }

    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    auc_scores = []

    for train_idx, val_idx in cv.split(X_global, y_global):
        X_train_fold, X_val_fold = X_global.iloc[train_idx], X_global.iloc[val_idx]
        y_train_fold, y_val_fold = y_global.iloc[train_idx], y_global.iloc[val_idx]

        model = XGBClassifier(**params)
        model.fit(
            X_train_fold,
            y_train_fold,
            eval_set=[(X_val_fold, y_val_fold)],
            verbose=0,
        )
        y_pred = model.predict_proba(X_val_fold)[:, 1]
        auc = roc_auc_score(y_val_fold, y_pred)
        auc_scores.append(auc)

    return np.mean(auc_scores)

def main():
    global X_global, y_global

    print("ğŸ”„ æ­£åœ¨è®€å– historical_races.csv...")
    df = pd.read_csv("historical_races.csv")
    print(f"ğŸ“Š åŸå§‹è³‡æ–™å½¢ç‹€: {df.shape}")

    # ç§»é™¤éæ•¸å€¼/éå¿…è¦æ¬„ä½
    cols_to_drop = ["race_date", "horse_name"]
    df = df.drop(columns=cols_to_drop, errors='ignore')
    print(f"âœ… å·²ç§»é™¤æ¬„ä½: {cols_to_drop}")

    # æª¢æŸ¥ç›®æ¨™è®Šæ•¸
    if "is_top3" not in df.columns:
        raise ValueError("âŒ ç¼ºå°‘ 'is_top3' æ¬„ä½ï¼")

    df = df.rename(columns={"is_top3": "top3"})
    y = df["top3"]
    X = df.drop(columns=["top3"])

    # ç·¨ç¢¼é¡åˆ¥è®Šæ•¸
    categorical_cols = ["jockey", "trainer", "track_condition", "class"]
    encoders = {}
    for col in categorical_cols:
        if col in X.columns:
            le = LabelEncoder()
            X[col] = X[col].fillna("æœªçŸ¥").astype(str)
            X[col] = le.fit_transform(X[col])
            encoders[col] = le

    # ç¢ºä¿ç„¡ object å‹æ…‹
    object_cols = X.select_dtypes(include=['object']).columns.tolist()
    if object_cols:
        raise ValueError(f"âŒ ä»æœ‰ object å‹æ¬„ä½: {object_cols}")

    print(f"âœ… æœ€çµ‚ç‰¹å¾µçŸ©é™£å½¢ç‹€: {X.shape}")
    print("ä½¿ç”¨ç‰¹å¾µ:", list(X.columns))

    # è¨­å®šå…¨åŸŸè®Šæ•¸ä¾› Optuna ä½¿ç”¨
    X_global = X
    y_global = y

    # ====== 1. è¶…åƒæ•¸èª¿å„ª ======
    print("\nğŸ” é–‹å§‹ Optuna è¶…åƒæ•¸èª¿å„ªï¼ˆç›®æ¨™ï¼šæœ€å¤§åŒ– AUCï¼‰...")
    study = optuna.create_study(direction="maximize")
    study.optimize(objective, n_trials=50)  # å¯èª¿æ•´è©¦é©—æ¬¡æ•¸ï¼ˆå»ºè­° 30ï½100ï¼‰

    print(f"\nğŸ¯ æœ€ä½³ AUC: {study.best_value:.4f}")
    print("æœ€ä½³åƒæ•¸:")
    for k, v in study.best_params.items():
        print(f"  {k}: {v}")

    # ====== 2. ç”¨æœ€ä½³åƒæ•¸è¨“ç·´æœ€çµ‚æ¨¡å‹ ======
    best_params = study.best_params
    best_params.update({
        "random_state": 42,
        "eval_metric": "logloss",
        "use_label_encoder": False,
    })

    print("\nğŸš€ ä½¿ç”¨æœ€ä½³åƒæ•¸è¨“ç·´æœ€çµ‚æ¨¡å‹...")
    final_model = XGBClassifier(**best_params)
    final_model.fit(X, y)

    # ====== 3. è©•ä¼°ï¼ˆå¯é¸ï¼šç”¨ç•™ä¸€æ³•æˆ–å…¨è³‡æ–™ AUCï¼‰=====
    y_pred_full = final_model.predict_proba(X)[:, 1]
    full_auc = roc_auc_score(y, y_pred_full)
    print(f"âœ… æœ€çµ‚æ¨¡å‹ AUC (å…¨è³‡æ–™): {full_auc:.4f}")

    # ====== 4. å„²å­˜æ¨¡å‹èˆ‡ç·¨ç¢¼å™¨ ======
    joblib.dump(final_model, "model.pkl")
    joblib.dump(encoders, "label_encoders.pkl")
    print("\nğŸ’¾ å·²å„²å­˜:")
    print("   - model.pkl")
    print("   - label_encoders.pkl")

    # ====== 5. è¼¸å‡ºç‰¹å¾µé‡è¦æ€§ ======
    feat_imp = final_model.feature_importances_
    features = X.columns
    indices = np.argsort(feat_imp)[::-1]

    print("\nğŸ” ç‰¹å¾µé‡è¦æ€§æ’åº:")
    for i in range(len(features)):
        print(f"  {i+1}. {features[indices[i]]}: {feat_imp[indices[i]]:.4f}")

    # ç¹ªåœ–
    plt.figure(figsize=(10, 6))
    plt.title("XGBoost Feature Importances")
    plt.bar(range(len(feat_imp)), feat_imp[indices], align="center")
    plt.xticks(range(len(feat_imp)), [features[i] for i in indices], rotation=45, ha='right')
    plt.tight_layout()
    plt.savefig("feature_importance.png", dpi=150)
    print("\nğŸ“ˆ ç‰¹å¾µé‡è¦æ€§åœ–å·²å„²å­˜ç‚º feature_importance.png")

if __name__ == "__main__":
    main()