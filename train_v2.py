#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è³½é¦¬ã€Œæ˜¯å¦é€²å…¥å‰ä¸‰åã€é æ¸¬æ¨¡å‹è¨“ç·´è…³æœ¬ (v2)
è¼¸å…¥ï¼šdata/historical_races_with_features.csv
è¼¸å‡ºï¼š
  - models/race_model_v2.pkl
  - data/predictions_train.csv
  - plots/feature_importance_v2.png
"""

import pandas as pd
import numpy as np
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
import xgboost as xgb

# ===== è¨­å®šè·¯å¾‘ =====
INPUT_DATA = "data/historical_races_with_features.csv"
MODEL_SAVE_PATH = "models/race_model_v2.pkl"
PREDICTIONS_OUTPUT = "data/predictions_train.csv"
FEATURE_IMPORTANCE_PLOT = "plots/feature_importance_v2.png"

# ===== ç‰¹å¾µé¸æ“‡ =====
FEATURES = [
    'draw',                      # æª”ä½
    'actual_weight',            # å¯¦éš›è² ç£…
    'win_odds',                 # ç¨è´è³ ç‡
    'race_distance',            # è·é›¢
    'last_is_top3',             # ä¸Šå ´æ˜¯å¦å‰ä¸‰
    'top3_rate_last_1',         # è¿‘1å ´å‰ä¸‰ç‡
    'top3_rate_last_3',         # è¿‘3å ´å‰ä¸‰ç‡
    'top3_rate_last_5',         # è¿‘5å ´å‰ä¸‰ç‡
    'avg_odds_last_3',          # è¿‘3å ´å¹³å‡è³ ç‡
    'avg_actual_weight_last_3', # è¿‘3å ´å¹³å‡è² ç£…
    'days_since_last_race'      # è·é›¢ä¸Šå ´å¤©æ•¸
]

TARGET = 'is_top3'

def prepare_data(df):
    """æº–å‚™è¨“ç·´è³‡æ–™ï¼šé¸å–ç‰¹å¾µã€è™•ç†ç¼ºå¤±å€¼"""
    # é¸å–ç‰¹å¾µèˆ‡ç›®æ¨™
    X = df[FEATURES].copy()
    y = df[TARGET].copy()
    
    # è™•ç†ç¼ºå¤±å€¼ï¼šæ•¸å€¼å‹ç”¨ä¸­ä½æ•¸å¡«è£œ
    imputer = SimpleImputer(strategy='median')
    X_imputed = pd.DataFrame(
        imputer.fit_transform(X),
        columns=X.columns,
        index=X.index
    )
    
    return X_imputed, y, imputer

def train_model(X_train, y_train, model_type='rf'):
    """è¨“ç·´æŒ‡å®šæ¨¡å‹"""
    if model_type == 'rf':
        model = RandomForestClassifier(
            n_estimators=200,
            max_depth=10,
            min_samples_split=10,
            random_state=42,
            class_weight='balanced'  # è™•ç†ä¸å¹³è¡¡é¡åˆ¥
        )
    elif model_type == 'xgb':
        model = xgb.XGBClassifier(
            n_estimators=200,
            max_depth=6,
            learning_rate=0.1,
            random_state=42,
            eval_metric='logloss'
        )
    elif model_type == 'lr':
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        model = LogisticRegression(
            class_weight='balanced',
            max_iter=1000,
            random_state=42
        )
        model.fit(X_train_scaled, y_train)
        return model, scaler
        # æ³¨æ„ï¼šLogisticRegression éœ€è¦å–®ç¨è™•ç† scaler
    else:
        raise ValueError("model_type must be 'rf', 'xgb', or 'lr'")
    
    model.fit(X_train, y_train)
    return model, None

def evaluate_model(model, X_test, y_test, model_type='rf', scaler=None):
    """è©•ä¼°æ¨¡å‹ä¸¦ç¹ªè£½ç‰¹å¾µé‡è¦æ€§"""
    # é æ¸¬
    if model_type == 'lr' and scaler is not None:
        X_test_scaled = scaler.transform(X_test)
        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
    else:
        y_pred_proba = model.predict_proba(X_test)[:, 1]
    
    y_pred = (y_pred_proba >= 0.5).astype(int)
    
    # è©•ä¼°æŒ‡æ¨™
    auc = roc_auc_score(y_test, y_pred_proba)
    print(f"\nâœ… æ¨¡å‹è©•ä¼°çµæœ (AUC): {auc:.4f}")
    print("\nåˆ†é¡å ±å‘Š:")
    print(classification_report(y_test, y_pred))
    
    # ç‰¹å¾µé‡è¦æ€§
    if model_type in ['rf', 'xgb']:
        importances = model.feature_importances_
    elif model_type == 'lr':
        importances = np.abs(model.coef_[0])
    
    feature_imp = pd.DataFrame({
        'feature': FEATURES,
        'importance': importances
    }).sort_values('importance', ascending=False)
    
    # ç¹ªåœ–
    plt.figure(figsize=(10, 6))
    sns.barplot(data=feature_imp.head(10), x='importance', y='feature')
    plt.title(f'Top 10 Feature Importance ({model_type.upper()})')
    plt.tight_layout()
    Path("plots").mkdir(exist_ok=True)
    plt.savefig(FEATURE_IMPORTANCE_PLOT, dpi=150)
    plt.close()
    print(f"ğŸ“Š ç‰¹å¾µé‡è¦æ€§åœ–å·²å„²å­˜è‡³: {FEATURE_IMPORTANCE_PLOT}")
    
    return y_pred_proba, feature_imp

def main():
    # å»ºç«‹ç›®éŒ„
    Path("models").mkdir(exist_ok=True)
    Path("plots").mkdir(exist_ok=True)
    
    # è®€å–è³‡æ–™
    print(f"è®€å–è¨“ç·´è³‡æ–™: {INPUT_DATA}")
    if not Path(INPUT_DATA).exists():
        print("âŒ è«‹å…ˆåŸ·è¡Œ feature_engineering.py ç”Ÿæˆç‰¹å¾µæª”æ¡ˆï¼")
        return
    
    df = pd.read_csv(INPUT_DATA)
    print(f"ç¸½ç­†æ•¸: {len(df)}, æ­£æ¨£æœ¬æ¯”ä¾‹: {df[TARGET].mean():.2%}")
    
    # æº–å‚™è³‡æ–™
    X, y, imputer = prepare_data(df)
    
    # åˆ‡åˆ†è¨“ç·´/æ¸¬è©¦é›†ï¼ˆæŒ‰æ—¥æœŸï¼Ÿé€™è£¡å…ˆéš¨æ©Ÿåˆ‡åˆ†ï¼‰
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # è¨“ç·´æ¨¡å‹ï¼ˆå¯æ”¹ç‚º 'xgb' æˆ– 'lr'ï¼‰
    MODEL_TYPE = 'rf'  # â† å¯åœ¨æ­¤åˆ‡æ›æ¨¡å‹
    print(f"\nğŸš€ é–‹å§‹è¨“ç·´ {MODEL_TYPE.upper()} æ¨¡å‹...")
    
    if MODEL_TYPE == 'lr':
        model, scaler = train_model(X_train, y_train, MODEL_TYPE)
        joblib.dump((model, scaler, imputer, FEATURES), MODEL_SAVE_PATH)
    else:
        model, _ = train_model(X_train, y_train, MODEL_TYPE)
        joblib.dump((model, None, imputer, FEATURES), MODEL_SAVE_PATH)
    
    print(f"âœ… æ¨¡å‹å·²å„²å­˜è‡³: {MODEL_SAVE_PATH}")
    
    # è©•ä¼°
    y_pred_proba, feature_imp = evaluate_model(
        model, X_test, y_test, MODEL_TYPE,
        scaler if MODEL_TYPE == 'lr' else None
    )
    
    # å„²å­˜è¨“ç·´é›†é æ¸¬çµæœï¼ˆä¾›åƒ¹å€¼æŠ•æ³¨åˆ†æï¼‰
    if MODEL_TYPE == 'lr':
        X_full_scaled = scaler.transform(X)
        full_pred_proba = model.predict_proba(X_full_scaled)[:, 1]
    else:
        full_pred_proba = model.predict_proba(X)[:, 1]
    
    df_pred = df.copy()
    df_pred['predicted_top3_prob'] = full_pred_proba
    df_pred.to_csv(PREDICTIONS_OUTPUT, index=False, encoding='utf-8')
    print(f"ğŸ’¾ å®Œæ•´é æ¸¬çµæœå·²å„²å­˜è‡³: {PREDICTIONS_OUTPUT}")
    
    # é¡¯ç¤º top 5 é‡è¦ç‰¹å¾µ
    print("\nğŸ† Top 5 é‡è¦ç‰¹å¾µ:")
    print(feature_imp.head().to_string(index=False))

if __name__ == "__main__":
    main()